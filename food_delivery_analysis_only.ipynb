{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02020fd9",
   "metadata": {},
   "source": [
    "# 📊 Food Delivery - Analysis Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a25ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load CSV into Spark DataFrame\n",
    "df = spark.read.csv(\"/FileStore/tables/24MBMA47_order.csv\", header=True, inferSchema=True)\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"orders\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4eecc0",
   "metadata": {},
   "source": [
    "## 🔍 Analysis Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Total Order Value per Customer\n",
    "spark.sql(\"\"\"\n",
    "SELECT customer_id, SUM(order_amount) AS total_spent\n",
    "FROM orders\n",
    "GROUP BY customer_id\n",
    "ORDER BY total_spent DESC\n",
    "\"\"\" ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e34d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Average Delivery Time per Partner\n",
    "from pyspark.sql.functions import unix_timestamp, avg\n",
    "df2 = df.withColumn(\"delivery_duration\",\n",
    "                    unix_timestamp(\"delivery_time\") - unix_timestamp(\"order_time\"))\n",
    "df2.groupBy(\"delivery_partner_id\").agg(avg(\"delivery_duration\").alias(\"avg_delivery_time\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Customers with Frequent Cancellations\n",
    "spark.sql(\"\"\"\n",
    "SELECT customer_id, COUNT(*) AS cancellations\n",
    "FROM orders\n",
    "WHERE order_status = 'Cancelled'\n",
    "GROUP BY customer_id\n",
    "HAVING COUNT(*) > 1\n",
    "\"\"\" ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Peak Order Hours\n",
    "spark.sql(\"\"\"\n",
    "SELECT HOUR(order_time) AS order_hour, COUNT(*) AS total_orders\n",
    "FROM orders\n",
    "GROUP BY HOUR(order_time)\n",
    "ORDER BY total_orders DESC\n",
    "\"\"\" ).show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}